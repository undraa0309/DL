# -*- coding: utf-8 -*-
"""Copy of Copy of main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OQuXBl0WEpqwv7wVYMByNNO5GRHg9wiI
"""

import torch

USE_CUDA = torch.cuda.is_available()
device = torch.device("cuda" if USE_CUDA else "cpu")

from google.colab import files
uploaded = files.upload()

from google.colab import files
uploaded = files.upload()

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader

import dataset
from model import LeNet5, CustomMLP

def train(model, trn_loader, device, criterion, optimizer):
    """ Train function

    Args:
        model: 네트워크 모델
        trn_loader: 트레이닝 데이터 로더
        device: 계산을 수행할 장치 (CPU 또는 GPU)
        criterion: 손실 함수
        optimizer: 최적화 방법

    Returns:
        trn_loss: 평균 손실 값
        acc: 정확도
    """
    model.train()  # 모델을 학습 모드로 설정
    running_loss = 0.0
    correct = 0
    total = 0

    for i, (inputs, labels) in enumerate(trn_loader):
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()  # 그라디언트 초기화
        outputs = model(inputs)  # 입력을 모델에 통과시켜 출력을 얻음
        loss = criterion(outputs, labels)
        loss.backward()  # 역전파 실행
        optimizer.step()  # 최적화 단계 진행

        running_loss += loss.item()
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

    trn_loss = running_loss / len(trn_loader)
    acc = 100 * correct / total
    return trn_loss, acc

def test(model, tst_loader, device, criterion):
    """ Test function

    Args:
        model: 네트워크 모델
        tst_loader: 테스트 데이터 로더
        device: 계산을 수행할 장치 (CPU 또는 GPU)
        criterion: 손실 함수

    Returns:
        tst_loss: 평균 손실 값
        acc: 정확도
    """
    model.eval()  # 모델을 평가 모드로 설정
    running_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():  # 그라디언트 계산 비활성화
        for inputs, labels in tst_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)

            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    tst_loss = running_loss / len(tst_loader)
    acc = 100 * correct / total
    return tst_loss, acc

def main():
    # Device 설정
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # 데이터셋 및 DataLoader 인스턴스 생성
    trn_dataset = dataset.MNIST('/mnt/data/train/train')
    tst_dataset = dataset.MNIST('/mnt/data/test/test')
    trn_loader = DataLoader(trn_dataset, batch_size=64, shuffle=True)
    tst_loader = DataLoader(tst_dataset, batch_size=64, shuffle=False)

    # 모델 생성 및 Device 할당
    model = LeNet5().to(device)

    # 손실 함수 및 최적화 방법 설정
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

    # 학습 및 테스트 실행
    for epoch in range(10):  # 에포크 수 설정
        trn_loss, trn_acc = train(model, trn_loader, device, criterion, optimizer)
        tst_loss, tst_acc = test(model, tst_loader, device, criterion)

        print(f'Epoch {epoch+1}:')
        print(f'Train Loss: {trn_loss:.4f}, Train Accuracy: {trn_acc:.2f}%')
        print(f'Test Loss: {tst_loss:.4f}, Test Accuracy: {tst_acc:.2f}%')

if __name__ == '__main__':
    main()

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader

import dataset
from model import LeNet5, CustomMLP

def train(model, trn_loader, device, criterion, optimizer):
    """ Train function

    Args:
        model: 네트워크 모델
        trn_loader: 트레이닝 데이터 로더
        device: 계산을 수행할 장치 (CPU 또는 GPU)
        criterion: 손실 함수
        optimizer: 최적화 방법

    Returns:
        trn_loss: 평균 손실 값
        acc: 정확도
    """
    model.train()  # 모델을 학습 모드로 설정
    running_loss = 0.0
    correct = 0
    total = 0

    for i, (inputs, labels) in enumerate(trn_loader):
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()  # 그라디언트 초기화
        outputs = model(inputs)  # 입력을 모델에 통과시켜 출력을 얻음
        loss = criterion(outputs, labels)
        loss.backward()  # 역전파 실행
        optimizer.step()  # 최적화 단계 진행

        running_loss += loss.item()
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

    trn_loss = running_loss / len(trn_loader)
    acc = 100 * correct / total
    return trn_loss, acc

def test(model, tst_loader, device, criterion):
    """ Test function

    Args:
        model: 네트워크 모델
        tst_loader: 테스트 데이터 로더
        device: 계산을 수행할 장치 (CPU 또는 GPU)
        criterion: 손실 함수

    Returns:
        tst_loss: 평균 손실 값
        acc: 정확도
    """
    model.eval()  # 모델을 평가 모드로 설정
    running_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():  # 그라디언트 계산 비활성화
        for inputs, labels in tst_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)

            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    tst_loss = running_loss / len(tst_loader)
    acc = 100 * correct / total
    return tst_loss, acc

def main():
    # Device 설정
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # 데이터셋 및 DataLoader 인스턴스 생성
    trn_dataset = dataset.MNIST('/mnt/data/train/train')
    tst_dataset = dataset.MNIST('/mnt/data/test/test')
    trn_loader = DataLoader(trn_dataset, batch_size=64, shuffle=True)
    tst_loader = DataLoader(tst_dataset, batch_size=64, shuffle=False)

    # 모델 생성 및 Device 할당
    model = CustomMLP().to(device)

    # 손실 함수 및 최적화 방법 설정
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

    # 학습 및 테스트 실행
    for epoch in range(10):  # 에포크 수 설정
        trn_loss, trn_acc = train(model, trn_loader, device, criterion, optimizer)
        tst_loss, tst_acc = test(model, tst_loader, device, criterion)

        print(f'Epoch {epoch+1}:')
        print(f'Train Loss: {trn_loss:.4f}, Train Accuracy: {trn_acc:.2f}%')
        print(f'Test Loss: {tst_loss:.4f}, Test Accuracy: {tst_acc:.2f}%')

if __name__ == '__main__':
    main()

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt

# 필요한 파일들을 임포트한다.
import sys
sys.path.append('/mnt/data')

import dataset
from model import LeNet5

def train(model, trn_loader, device, criterion, optimizer):
    model.train()  # 모델을 학습 모드로 설정
    running_loss = 0.0
    correct = 0
    total = 0

    for i, (inputs, labels) in enumerate(trn_loader):
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()  # 그라디언트 초기화
        outputs = model(inputs)  # 입력을 모델에 통과시켜 출력을 얻음
        loss = criterion(outputs, labels)
        loss.backward()  # 역전파 실행
        optimizer.step()  # 최적화 단계 진행

        running_loss += loss.item()
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

    trn_loss = running_loss / len(trn_loader)
    acc = 100 * correct / total
    return trn_loss, acc

def test(model, tst_loader, device, criterion):
    model.eval()  # 모델을 평가 모드로 설정
    running_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():  # 그라디언트 계산 비활성화
        for inputs, labels in tst_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)

            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    tst_loss = running_loss / len(tst_loader)
    acc = 100 * correct / total
    return tst_loss, acc

# 결과를 저장할 리스트 초기화
train_losses = []
train_accuracies = []
test_losses = []
test_accuracies = []

# Device 설정
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 데이터셋 및 DataLoader 인스턴스 생성
trn_dataset = dataset.MNIST('/mnt/data/train/train')
tst_dataset = dataset.MNIST('/mnt/data/test/test')
trn_loader = DataLoader(trn_dataset, batch_size=64, shuffle=True)
tst_loader = DataLoader(tst_dataset, batch_size=64, shuffle=False)

# 모델 생성 및 Device 할당
model = LeNet5().to(device)

# 손실 함수 및 최적화 방법 설정
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

# 학습 및 테스트 실행
for epoch in range(10):  # 에포크 수 설정
    trn_loss, trn_acc = train(model, trn_loader, device, criterion, optimizer)
    tst_loss, tst_acc = test(model, tst_loader, device, criterion)

    # 결과 저장
    train_losses.append(trn_loss)
    train_accuracies.append(trn_acc)
    test_losses.append(tst_loss)
    test_accuracies.append(tst_acc)

# 결과 그래프 그리기
epochs = range(1, 11)
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.plot(epochs, train_losses, 'r-', label='Train Loss')
plt.plot(epochs, test_losses, 'b-', label='Test Loss')
plt.title('Loss over epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(epochs, train_accuracies, 'r-', label='Train Accuracy')
plt.plot(epochs, test_accuracies, 'b-', label='Test Accuracy')
plt.title('Accuracy over epochs')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader

import dataset
from model import LeNet5, CustomMLP

def train(model, trn_loader, device, criterion, optimizer):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for inputs, labels in trn_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

    trn_loss = running_loss / len(trn_loader)
    acc = 100.0 * correct / total
    return trn_loss, acc

def test(model, tst_loader, device, criterion):
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():
        for inputs, labels in tst_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)

            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    tst_loss = running_loss / len(tst_loader)
    acc = 100.0 * correct / total
    return tst_loss, acc

def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # 데이터 로더 설정
    trn_dataset = dataset.MNIST('/mnt/data/train/train')  # 정확한 경로로 수정
    tst_dataset = dataset.MNIST('/mnt/data/test/test')  # 정확한 경로로 수정
    trn_loader = DataLoader(trn_dataset, batch_size=64, shuffle=True)
    tst_loader = DataLoader(tst_dataset, batch_size=64, shuffle=False)

    # LeNet-5 모델 훈련
    lenet_model = LeNet5().to(device)
    optimizer = optim.SGD(lenet_model.parameters(), lr=0.01, momentum=0.9)
    criterion = nn.CrossEntropyLoss()

    for epoch in range(10):  # 10 에포크로 설정
        trn_loss, trn_acc = train(lenet_model, trn_loader, device, criterion, optimizer)
        print(f'Epoch {epoch+1}, LeNet-5 - Train Loss: {trn_loss:.4f}, Accuracy: {trn_acc:.2f}%')

    # Custom MLP 모델 훈련
    mlp_model = CustomMLP().to(device)
    optimizer = optim.SGD(mlp_model.parameters(), lr=0.01, momentum=0.9)

    for epoch in range(10):  # 10 에포크로 설정
        trn_loss, trn_acc = train(mlp_model, trn_loader, device, criterion, optimizer)
        print(f'Epoch {epoch+1}, Custom MLP - Train Loss: {trn_loss:.4f}, Accuracy: {trn_acc:.2f}%')

if __name__ == '__main__':
    main()

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader

import dataset
from model import LeNet5, CustomMLP

def train(model, trn_loader, device, criterion, optimizer):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for inputs, labels in trn_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

    trn_loss = running_loss / len(trn_loader)
    acc = 100.0 * correct / total
    return trn_loss, acc

def test(model, tst_loader, device, criterion):
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():
        for inputs, labels in tst_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)

            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    tst_loss = running_loss / len(tst_loader)
    acc = 100.0 * correct / total
    return tst_loss, acc

def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # 데이터 로더 설정
    trn_dataset = dataset.MNIST('/mnt/data/train/train')  # 정확한 경로로 수정
    tst_dataset = dataset.MNIST('/mnt/data/test/test')  # 정확한 경로로 수정
    trn_loader = DataLoader(trn_dataset, batch_size=64, shuffle=True)
    tst_loader = DataLoader(tst_dataset, batch_size=64, shuffle=False)

    # LeNet-5 모델 훈련
    lenet_model = LeNet5().to(device)
    optimizer = optim.SGD(lenet_model.parameters(), lr=0.01, momentum=0.9)
    criterion = nn.CrossEntropyLoss()

    for epoch in range(10):  # 10 에포크로 설정
        tst_loss, tst_acc = train(lenet_model, tst_loader, device, criterion, optimizer)
        print(f'Epoch {epoch+1}, LeNet-5 - Test Loss: {tst_loss:.4f}, Accuracy: {tst_acc:.2f}%')

    # Custom MLP 모델 훈련
    mlp_model = CustomMLP().to(device)
    optimizer = optim.SGD(mlp_model.parameters(), lr=0.01, momentum=0.9)

    for epoch in range(10):  # 10 에포크로 설정
        tst_loss, tst_acc = train(mlp_model, tst_loader, device, criterion, optimizer)
        print(f'Epoch {epoch+1}, Custom MLP - Test Loss: {tst_loss:.4f}, Accuracy: {tst_acc:.2f}%')

if __name__ == '__main__':
    main()

